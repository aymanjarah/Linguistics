{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from nltk.tokenize import word_tokenize\n",
    "from iteration_utilities import deepflatten\n",
    "from nltk.util import ngrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus name\n",
    "corpus='US2016tv'\n",
    "# Path of folder containing the corpus\n",
    "directory = os.fsencode(r'C:/Users/Aymane-JARAH/Desktop/Thesis/'+corpus)\n",
    "list_files=[]\n",
    "appended_dataframe = []\n",
    "for file in os.listdir(directory):\n",
    "     filename = os.fsdecode(file)\n",
    "     if filename.endswith(\".json\"): \n",
    "          appended_dataframe.append(create_dataframe(filename,corpus))\n",
    "     else:\n",
    "         continue\n",
    "appended_data = pd.concat(appended_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(filename,corpus):\n",
    "    with open(r'C:/Users/Aymane-JARAH/Desktop/Thesis/'+corpus+'/'+filename) as f:\n",
    "        json_node = json.load(f) \n",
    "    get_nodes_with_question_type(json_node)\n",
    "    nodes_check=[d['nodeID'] for d in get_nodes_with_question_type(json_node)]\n",
    "    questions=[d['fromID'] for d in list(filter(lambda x: (x['toID'] in nodes_check), json_node['edges']))]\n",
    "    \n",
    "    question_types=[d['toID'] for d in list(filter(lambda x: (x['fromID'] in questions), json_node['edges']))]\n",
    "    question_types=list(filter(lambda x: (get_text_from_node_id(x,json_node) in list_), question_types))\n",
    "    \n",
    "    data_nodes_id=pd.DataFrame({'Types':[get_text_from_node_id(node_id,json_node) for node_id in question_types],\n",
    "                            'Questions':[get_text_from_node_id(node_id,json_node) for node_id in questions],\n",
    "                            'nodeID': [node_id for node_id in questions],\n",
    "                            'PersonID':[get_text_from_node_id(node_id,json_node).split(':')[0] for node_id in questions],\n",
    "                            'Precedents':[get_precedents(question,json_node) for question in questions],\n",
    "                            'Succesors':[get_succesors(question,json_node) for question in questions],\n",
    "                            'NodeSet':[filename]*len(questions)\n",
    "                           })\n",
    "    return data_nodes_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_node_id(node_id,json_file):\n",
    "    node = list(filter(lambda x: (x['nodeID'] == node_id), json_file['nodes']))\n",
    "    if node:\n",
    "        return node[0]['text']\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_with_question_type(json_nodes):\n",
    "    questions_nodes = list(filter(lambda x: (x['text'] in list_), json_nodes['nodes']))\n",
    "    return questions_nodes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person_from_node_id(node_id,json_file):\n",
    "    node = list(filter(lambda x: (x['nodeID'] == node_id), json_file['locutions']))\n",
    "    if node:\n",
    "        return node[0]['personID']\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_from_node_id(node_id,json_file):\n",
    "    node = list(filter(lambda x: (x['nodeID'] == node_id), json_file['nodes']))\n",
    "    if node:\n",
    "        return node[0]['type']\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precedents(question_id,json_file):\n",
    "    prec=[]\n",
    "    questions=list(filter(lambda x: (x['toID']==question_id), json_file['edges']))\n",
    "    if questions:\n",
    "        for q in questions:\n",
    "            \n",
    "            L_precedents=list(filter(lambda x: (x['toID'] in [q['fromID']]), json_file['edges']))\n",
    "            L_precedents=list(filter(lambda x: (get_type_from_node_id(x['fromID'],json_file)=='L'), L_precedents))\n",
    "            \n",
    "            if L_precedents:\n",
    "                prec.append(L_precedents)\n",
    "            \n",
    "    else:\n",
    "        prec=[]\n",
    "\n",
    "    return [{get_text_from_node_id(precedent[0]['fromID'],json_file).split(':')[0]:get_text_from_node_id(precedent[0]['fromID'],json_file)} for precedent in prec]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_succesors(question_id,json_file):\n",
    "    succ=[]\n",
    "    questions=list(filter(lambda x: (x['fromID']==question_id), json_file['edges']))\n",
    "    questions=list(filter(lambda x: (get_type_from_node_id(x['toID'],json_file)=='TA'), questions))\n",
    "    if questions:\n",
    "        for q in questions:\n",
    "            succ.append(list(filter(lambda x: (x['fromID'] in [q['toID']]), json_file['edges'])))       \n",
    "    else:\n",
    "        succ=[]\n",
    "    return [{get_text_from_node_id(succesor[0]['toID'],json_file).split(':')[0]:get_text_from_node_id(succesor[0]['toID'],json_file)} for succesor in succ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "appended_data['Succesors']=appended_data['Succesors'].apply(lambda l : [set(e.values()) for e in l])\n",
    "appended_data['Precedents']=appended_data['Precedents'].apply(lambda l : [set(e.values()) for e in l])\n",
    "appended_data.to_csv(corpus+'.csv', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
